{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77555c96-262e-4766-a384-eab5641273f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model selection\n",
        "API_KEY = \"your-openai-api-key-here\"\n",
        "MODEL = \"grok-3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da0091e0-fcb6-4839-8ef9-086c87d0baca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "import openai\n",
        "from rich.console import Console\n",
        "import time\n",
        "from openai import OpenAIError  \n",
        "\n",
        "console = Console()\n",
        "\n",
        "def chat_completion(messages, max_retries=3, backoff_factor=2):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            client = openai.OpenAI(\n",
        "                api_key=API_KEY,\n",
        "                base_url=\"https://api.x.ai/v1\"\n",
        "            )\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=messages,\n",
        "                temperature=0,\n",
        "                max_tokens=2048\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except OpenAIError as e:  \n",
        "            if e.status_code == 429:  \n",
        "                sleep_time = backoff_factor ** attempt  \n",
        "                console.print(f\"[bold yellow]Rate limit hit (429). Retrying after {sleep_time} seconds... (Attempt {attempt+1}/{max_retries})[/]\")\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                console.print(f\"[bold red]API request failed: {str(e)}[/]\")\n",
        "                return None\n",
        "    console.print(f\"[bold red]Max retries reached for API call.[/]\")\n",
        "    return None\n",
        "\n",
        "# Function to read PMIDs from a text file (ç»Ÿä¸€ä½¿ç”¨æ­¤ç‰ˆæœ¬)\n",
        "def read_pmid_from_txt(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            pmids = [line.strip() for line in file if line.strip()]\n",
        "        return pmids\n",
        "    except FileNotFoundError:\n",
        "        console.print(f\"[bold red]Error: File {file_path} not found.[/]\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        console.print(f\"[bold red]Error reading {file_path}: {str(e)}[/]\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3e5ba29-cfa8-437b-b1a3-50e9b00b8a71",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# PubMedå…ƒæ•°æ®æŠ“å–æ¨¡å—\n",
        "# =====================================\n",
        "# åŠŸèƒ½: ä»PubMedé¡µé¢æŠ“å–æ–‡ç« å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€DOIã€å…³é”®è¯ç­‰ï¼‰\n",
        "# æ³¨æ„: PMCå…¨æ–‡æŠ“å–è¯·ä½¿ç”¨ä¸‹é¢çš„ä¼˜åŒ–ç‰ˆæœ¬ (v2.1)\n",
        "# =====================================\n",
        "\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Setup the web driver for scraping\n",
        "def setup_driver():\n",
        "    \"\"\"é…ç½®Selenium WebDriverç”¨äºPubMedé¡µé¢æŠ“å–\"\"\"\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124\")\n",
        "    chrome_options.add_argument('--disable-gpu')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "# Fetch data from PubMed using the provided PMID\n",
        "def fetch_pubmed_data(pmid, driver):\n",
        "    \"\"\"\n",
        "    ä»PubMedé¡µé¢æŠ“å–æ–‡ç« å…ƒæ•°æ®\n",
        "    \n",
        "    Args:\n",
        "        pmid: PubMedæ–‡ç« ID\n",
        "        driver: Selenium WebDriverå®ä¾‹\n",
        "    \n",
        "    Returns:\n",
        "        dict: åŒ…å«æ ‡é¢˜ã€ä½œè€…ã€DOIã€å…³é”®è¯ã€æœŸåˆŠåã€PMCIDç­‰å…ƒæ•°æ®\n",
        "    \"\"\"\n",
        "    url = f\"https://www.ncbi.nlm.nih.gov/pubmed/{pmid}\"\n",
        "    driver.get(url)\n",
        "    \n",
        "    data = {'pmid': pmid}\n",
        "    wait = WebDriverWait(driver, 5)\n",
        "    \n",
        "    try:\n",
        "        data['title'] = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'heading-title'))).text.strip()\n",
        "    except:\n",
        "        data['title'] = None\n",
        "\n",
        "    try:\n",
        "        authors = driver.find_element(By.CLASS_NAME, 'authors-list').text\n",
        "        data['authors'] = ', '.join(re.sub(r'\\s*\\d+', '', authors).split(','))\n",
        "    except:\n",
        "        data['authors'] = None\n",
        "\n",
        "    try:\n",
        "        affiliation = driver.find_element(By.CLASS_NAME, 'affiliation-link').get_attribute('title')\n",
        "        data['first_author_last_affiliation_word'] = affiliation.split(',')[-1].strip().split()[-1].rstrip('.')\n",
        "    except:\n",
        "        data['first_author_last_affiliation_word'] = None\n",
        "\n",
        "    try:\n",
        "        data['doi'] = driver.find_element(By.XPATH, '//a[@data-ga-action=\"DOI\"]').text.strip()\n",
        "    except:\n",
        "        data['doi'] = None\n",
        "\n",
        "    try:\n",
        "        keywords = driver.find_element(By.XPATH, '//p[strong[contains(text(),\"Keywords\")]]').text\n",
        "        data['keywords'] = keywords.replace(\"Keywords:\", \"\").strip().rstrip('.')\n",
        "    except:\n",
        "        data['keywords'] = None\n",
        "\n",
        "    try:\n",
        "        data['journal_name'] = driver.find_element(By.XPATH, '//meta[@name=\"citation_publisher\"]').get_attribute('content').strip()\n",
        "    except:\n",
        "        data['journal_name'] = None\n",
        "\n",
        "    try:\n",
        "        pmcid_element = driver.find_element(By.XPATH, '//a[contains(@href, \"pmc.ncbi.nlm.nih.gov/articles/PMC\")]')\n",
        "        pmcid_full = pmcid_element.text.strip()\n",
        "        data['pmcid'] = pmcid_full.replace(\"PMCID: \", \"\").strip()\n",
        "    except:\n",
        "        data['pmcid'] = None\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efee0fb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# ğŸ”§ ä¼˜åŒ–åçš„PMCå…¨æ–‡æŠ“å–æ¨¡å— (v2.1)\n",
        "# =====================================\n",
        "# åŠŸèƒ½æ”¹è¿›:\n",
        "# 1. ç²¾ç¡®æå–æ­£æ–‡å†…å®¹ï¼Œæ’é™¤References/Footnotesç­‰éæ­£æ–‡ç« èŠ‚\n",
        "# 2. ä¸‰å±‚fallbackæœºåˆ¶ï¼Œæé«˜æŠ“å–æˆåŠŸç‡\n",
        "# 3. å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•\n",
        "# 4. ä¸“é—¨é€‚é… https://pmc.ncbi.nlm.nih.gov/articles/PMC{ID}/ é¡µé¢ç»“æ„\n",
        "# 5. æ”¯æŒmain-article-bodyç²¾ç¡®å®šä½\n",
        "# =====================================\n",
        "\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from rich.console import Console\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List, Dict, Tuple\n",
        "from enum import Enum\n",
        "import traceback\n",
        "\n",
        "# å¯é€‰: Seleniumæ”¯æŒ\n",
        "try:\n",
        "    from selenium import webdriver\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    from selenium.webdriver.common.by import By\n",
        "    from selenium.webdriver.support.ui import WebDriverWait\n",
        "    from selenium.webdriver.support import expected_conditions as EC\n",
        "    SELENIUM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SELENIUM_AVAILABLE = False\n",
        "    print(\"âš  Seleniumæœªå®‰è£…ï¼Œå°†ä»…ä½¿ç”¨requestsæŠ“å–\")\n",
        "\n",
        "console = Console()\n",
        "\n",
        "# ============== çŠ¶æ€ä¸ç»“æœç±» ==============\n",
        "\n",
        "class ExtractionStatus(Enum):\n",
        "    \"\"\"æŠ“å–çŠ¶æ€æšä¸¾\"\"\"\n",
        "    SUCCESS = \"success\"\n",
        "    PARTIAL = \"partial\"      # éƒ¨åˆ†æˆåŠŸï¼ˆä½¿ç”¨äº†fallbackï¼‰\n",
        "    FAILED = \"failed\"\n",
        "    NO_PMCID = \"no_pmcid\"\n",
        "\n",
        "@dataclass\n",
        "class ExtractionResult:\n",
        "    \"\"\"æŠ“å–ç»“æœæ•°æ®ç±»\"\"\"\n",
        "    pmcid: str\n",
        "    status: ExtractionStatus\n",
        "    full_text: Optional[str] = None\n",
        "    full_text_chunks: Optional[List[str]] = None\n",
        "    error_message: Optional[str] = None\n",
        "    method_used: Optional[str] = None\n",
        "    sections_found: Optional[List[str]] = None\n",
        "    char_count: int = 0\n",
        "    word_count: int = 0\n",
        "\n",
        "# ============== é…ç½®å¸¸é‡ ==============\n",
        "\n",
        "# éœ€è¦æ’é™¤çš„ç« èŠ‚æ ‡é¢˜ï¼ˆåˆ°è¾¾è¿™äº›ç« èŠ‚æ—¶åœæ­¢æŠ“å–ï¼‰\n",
        "EXCLUDE_SECTION_PATTERNS = [\n",
        "    r'^references?$',\n",
        "    r'^bibliography$',\n",
        "    r'^acknowledgm?ents?$',\n",
        "    r'^acknowledg?ments?$',\n",
        "    r'^author\\s*contributions?$',\n",
        "    r'^authors?\\s*contributions?$',\n",
        "    r'^contributors?$',\n",
        "    r'^contributor\\s*information$',\n",
        "    r'^conflicts?\\s*of\\s*interests?$',\n",
        "    r'^competing\\s*interests?$',\n",
        "    r'^declarations?$',\n",
        "    r'^disclosure$',\n",
        "    r'^funding$',\n",
        "    r'^funding\\s*(sources?|information)?$',\n",
        "    r'^financial\\s*disclosure$',\n",
        "    r'^supplementary\\s*(materials?|data|information)?$',\n",
        "    r'^supporting\\s*information$',\n",
        "    r'^appendi(x|ces)$',\n",
        "    r'^data\\s*availability',\n",
        "    r'^ethics\\s*(statement|approval)?$',\n",
        "    r'^ethical\\s*approval$',\n",
        "    r'^footnotes?$',\n",
        "    r'^abbreviations?$',\n",
        "    r'^associated\\s*data$',\n",
        "]\n",
        "\n",
        "# æ­£æ–‡ç« èŠ‚æ ‡é¢˜ï¼ˆç”¨äºè¯†åˆ«æ­£æ–‡å¼€å§‹ä½ç½®ï¼‰\n",
        "MAIN_CONTENT_PATTERNS = [\n",
        "    r'^abstract$',\n",
        "    r'^highlights?$',\n",
        "    r'^background$',\n",
        "    r'^introduction$',\n",
        "    r'^methods?$',\n",
        "    r'^materials?\\s*(and|&)\\s*methods?$',\n",
        "    r'^patients?\\s*(and|&)\\s*methods?$',\n",
        "    r'^study\\s*design$',\n",
        "    r'^results?$',\n",
        "    r'^findings?$',\n",
        "    r'^discussion$',\n",
        "    r'^conclusions?$',\n",
        "    r'^summary$',\n",
        "]\n",
        "\n",
        "# ============== å·¥å…·å‡½æ•° ==============\n",
        "\n",
        "def is_exclude_section(title: str) -> bool:\n",
        "    \"\"\"æ£€æŸ¥ç« èŠ‚æ ‡é¢˜æ˜¯å¦åº”è¯¥è¢«æ’é™¤\"\"\"\n",
        "    if not title:\n",
        "        return False\n",
        "    title_clean = title.strip().lower()\n",
        "    for pattern in EXCLUDE_SECTION_PATTERNS:\n",
        "        if re.match(pattern, title_clean, re.IGNORECASE):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def is_main_content_section(title: str) -> bool:\n",
        "    \"\"\"æ£€æŸ¥ç« èŠ‚æ ‡é¢˜æ˜¯å¦ä¸ºæ­£æ–‡ç« èŠ‚\"\"\"\n",
        "    if not title:\n",
        "        return False\n",
        "    title_clean = title.strip().lower()\n",
        "    for pattern in MAIN_CONTENT_PATTERNS:\n",
        "        if re.match(pattern, title_clean, re.IGNORECASE):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def clean_text_v2(text: str) -> str:\n",
        "    \"\"\"æ¸…æ´—æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½ã€ORCIDã€é‚®ç®±ç­‰\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    \n",
        "    # å»é™¤ORCID ID\n",
        "    text = re.sub(r'ORCID\\s*(ID)?\\s*:\\s*\\S+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'https?://orcid\\.org/\\S+', '', text)\n",
        "    \n",
        "    # å»é™¤é‚®ç®±åœ°å€\n",
        "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
        "    \n",
        "    # å»é™¤Figure/Tableæ ‡æ³¨ä¸­çš„é‡å¤å†…å®¹\n",
        "    text = re.sub(r'(Figure|Table)\\s*\\d+\\.?\\s*(Open in a new tab)?', r'\\1 ', text, flags=re.I)\n",
        "    \n",
        "    # å»é™¤å¤šä½™ç©ºç™½\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "def chunk_text_v2(text: str, chunk_size_words: int = 4000, overlap_words: int = 1000) -> List[str]:\n",
        "    \"\"\"å°†æ–‡æœ¬åˆ†å—ï¼Œå¸¦é‡å \"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) <= chunk_size_words:\n",
        "        return [text]\n",
        "    \n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = min(start + chunk_size_words, len(words))\n",
        "        chunk = ' '.join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        if end == len(words):\n",
        "            break\n",
        "        start += chunk_size_words - overlap_words\n",
        "    return chunks\n",
        "\n",
        "# ============== PMCæŠ“å–å™¨ç±» ==============\n",
        "\n",
        "class PMCScraperV2:\n",
        "    \"\"\"\n",
        "    PMCå…¨æ–‡æŠ“å–å™¨ v2.1\n",
        "    \n",
        "    ä¸“é—¨é€‚é…PMCç½‘é¡µç»“æ„ï¼Œæ”¯æŒå¤šç§æŠ“å–ç­–ç•¥ï¼š\n",
        "    - æ–¹æ³•1: åŸºäºmain-article-bodyçš„ç²¾ç¡®æå–ï¼ˆæ¨èï¼‰\n",
        "    - æ–¹æ³•2: åŸºäºæ ‡é¢˜æ ‡ç­¾å®šä½çš„æå–\n",
        "    - æ–¹æ³•3: Fallbackå…¨æ–‡æå–åæ­£åˆ™æˆªæ–­\n",
        "    \n",
        "    ä½¿ç”¨ç¤ºä¾‹:\n",
        "        scraper = PMCScraperV2()\n",
        "        result = scraper.extract_full_text(\"PMC12372713\")\n",
        "        print(result.full_text)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, use_selenium: bool = False, timeout: int = 30):\n",
        "        self.use_selenium = use_selenium and SELENIUM_AVAILABLE\n",
        "        self.timeout = timeout\n",
        "        self.driver = None\n",
        "        self.request_headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'Connection': 'keep-alive',\n",
        "        }\n",
        "        \n",
        "    def setup_driver(self):\n",
        "        \"\"\"é…ç½®Selenium WebDriver\"\"\"\n",
        "        if not SELENIUM_AVAILABLE:\n",
        "            raise RuntimeError(\"Seleniumæœªå®‰è£…\")\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(f\"user-agent={self.request_headers['User-Agent']}\")\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "        return self.driver\n",
        "    \n",
        "    def close_driver(self):\n",
        "        \"\"\"å…³é—­WebDriver\"\"\"\n",
        "        if self.driver:\n",
        "            try:\n",
        "                self.driver.quit()\n",
        "            except:\n",
        "                pass\n",
        "            self.driver = None\n",
        "    \n",
        "    def fetch_html(self, pmcid: str) -> Tuple[Optional[str], Optional[str]]:\n",
        "        \"\"\"\n",
        "        è·å–PMCé¡µé¢HTML\n",
        "        \n",
        "        Returns:\n",
        "            Tuple[html_content, error_message]\n",
        "        \"\"\"\n",
        "        url = f\"https://pmc.ncbi.nlm.nih.gov/articles/{pmcid}/\"\n",
        "        \n",
        "        # æ–¹æ³•1: ä½¿ç”¨requestsï¼ˆæ›´å¿«ã€æ›´ç¨³å®šï¼‰\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.request_headers, timeout=self.timeout)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            # æ£€æŸ¥æ˜¯å¦è·å–åˆ°æœ‰æ•ˆå†…å®¹ï¼ˆPMCé¡µé¢é€šå¸¸>50KBï¼‰\n",
        "            if len(response.text) > 10000:\n",
        "                console.print(f\"[green]  âœ“ requestsæˆåŠŸ ({len(response.text)//1024}KB)[/]\")\n",
        "                return response.text, None\n",
        "            else:\n",
        "                console.print(f\"[yellow]  âš  é¡µé¢å†…å®¹è¿‡çŸ­ ({len(response.text)}B)[/]\")\n",
        "        except requests.Timeout:\n",
        "            console.print(f\"[yellow]  âš  requestsè¶…æ—¶ï¼Œå°è¯•Selenium...[/]\")\n",
        "        except requests.HTTPError as e:\n",
        "            if e.response.status_code == 404:\n",
        "                return None, f\"æ–‡ç« ä¸å­˜åœ¨ (404): {pmcid}\"\n",
        "            console.print(f\"[yellow]  âš  HTTPé”™è¯¯ {e.response.status_code}[/]\")\n",
        "        except requests.RequestException as e:\n",
        "            console.print(f\"[yellow]  âš  requestså¤±è´¥: {str(e)[:50]}[/]\")\n",
        "        \n",
        "        # æ–¹æ³•2: Selenium fallbackï¼ˆå¤„ç†JSæ¸²æŸ“é¡µé¢ï¼‰\n",
        "        if self.use_selenium:\n",
        "            try:\n",
        "                console.print(\"[cyan]  å°è¯•SeleniumæŠ“å–...[/]\")\n",
        "                if not self.driver:\n",
        "                    self.setup_driver()\n",
        "                self.driver.get(url)\n",
        "                WebDriverWait(self.driver, self.timeout).until(\n",
        "                    EC.presence_of_element_located((By.CLASS_NAME, 'main-article-body'))\n",
        "                )\n",
        "                time.sleep(1)  # ç­‰å¾…å†…å®¹å®Œå…¨åŠ è½½\n",
        "                html = self.driver.page_source\n",
        "                if len(html) > 10000:\n",
        "                    console.print(f\"[green]  âœ“ SeleniumæˆåŠŸ ({len(html)//1024}KB)[/]\")\n",
        "                    return html, None\n",
        "            except Exception as e:\n",
        "                console.print(f\"[red]  âœ— Seleniumå¤±è´¥: {str(e)[:50]}[/]\")\n",
        "                return None, f\"Seleniumå¤±è´¥: {str(e)}\"\n",
        "        \n",
        "        return None, \"æ— æ³•è·å–é¡µé¢å†…å®¹\"\n",
        "    \n",
        "    def extract_method1_main_body(self, soup: BeautifulSoup) -> Tuple[Optional[str], List[str]]:\n",
        "        \"\"\"\n",
        "        æ–¹æ³•1: åŸºäºmain-article-bodyçš„ç²¾ç¡®æå–ï¼ˆæ¨èï¼‰\n",
        "        \n",
        "        PMCé¡µé¢ç»“æ„:\n",
        "        <section class=\"body main-article-body\">\n",
        "            <section class=\"abstract\" id=\"abstract1\">...</section>\n",
        "            <section id=\"sec1\">...</section>  # Methods\n",
        "            <section id=\"sec2\">...</section>  # Results\n",
        "            ...\n",
        "        </section>\n",
        "        \"\"\"\n",
        "        sections_found = []\n",
        "        content_parts = []\n",
        "        \n",
        "        # æŸ¥æ‰¾ä¸»æ–‡ç« ä½“\n",
        "        main_body = soup.find('section', class_='main-article-body')\n",
        "        if not main_body:\n",
        "            # Fallback: å°è¯•å…¶ä»–å¯èƒ½çš„å®¹å™¨\n",
        "            main_body = soup.find('div', class_='article-content') or \\\n",
        "                        soup.find('article', class_='article')\n",
        "        \n",
        "        if not main_body:\n",
        "            return None, [\"æœªæ‰¾åˆ°main-article-body\"]\n",
        "        \n",
        "        console.print(f\"[dim]    æ‰¾åˆ°ä¸»æ–‡ç« å®¹å™¨[/]\")\n",
        "        \n",
        "        # éå†ä¸»ä½“å†…çš„æ‰€æœ‰section\n",
        "        for section in main_body.find_all('section', recursive=False):\n",
        "            # è·å–ç« èŠ‚æ ‡é¢˜\n",
        "            heading = section.find(['h2', 'h3', 'h4'])\n",
        "            section_title = heading.get_text(strip=True) if heading else \"\"\n",
        "            \n",
        "            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢\n",
        "            if section_title and is_exclude_section(section_title):\n",
        "                console.print(f\"[dim]    â¹ åœæ­¢äº: {section_title}[/]\")\n",
        "                break\n",
        "            \n",
        "            # æå–ç« èŠ‚å†…å®¹\n",
        "            section_text = section.get_text(separator=' ', strip=True)\n",
        "            if section_text and len(section_text) > 50:\n",
        "                content_parts.append(section_text)\n",
        "                if section_title:\n",
        "                    sections_found.append(section_title)\n",
        "                    console.print(f\"[dim]    âœ“ æå–: {section_title[:40]}... ({len(section_text)}å­—ç¬¦)[/]\")\n",
        "        \n",
        "        if content_parts:\n",
        "            return ' '.join(content_parts), sections_found\n",
        "        return None, sections_found\n",
        "    \n",
        "    def extract_method2_heading(self, soup: BeautifulSoup) -> Tuple[Optional[str], List[str]]:\n",
        "        \"\"\"\n",
        "        æ–¹æ³•2: åŸºäºæ ‡é¢˜æ ‡ç­¾å®šä½çš„æå–\n",
        "        \n",
        "        ä»Abstract/Introductionå¼€å§‹ï¼Œåˆ°References/Footnotesç»“æŸ\n",
        "        \"\"\"\n",
        "        sections_found = []\n",
        "        content_parts = []\n",
        "        headings = soup.find_all(['h2', 'h3'])\n",
        "        start_found = False\n",
        "        \n",
        "        for heading in headings:\n",
        "            heading_text = heading.get_text(strip=True)\n",
        "            \n",
        "            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æ’é™¤ç« èŠ‚\n",
        "            if is_exclude_section(heading_text):\n",
        "                console.print(f\"[dim]    â¹ åœæ­¢äº: {heading_text}[/]\")\n",
        "                break\n",
        "            \n",
        "            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æ­£æ–‡å¼€å§‹\n",
        "            if is_main_content_section(heading_text):\n",
        "                start_found = True\n",
        "            \n",
        "            if start_found:\n",
        "                sections_found.append(heading_text)\n",
        "                \n",
        "                # æå–è¯¥æ ‡é¢˜åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ä¹‹é—´çš„å†…å®¹\n",
        "                content = []\n",
        "                current = heading.find_next_sibling()\n",
        "                while current:\n",
        "                    if current.name in ['h2', 'h3']:\n",
        "                        break\n",
        "                    text = current.get_text(separator=' ', strip=True)\n",
        "                    if text:\n",
        "                        content.append(text)\n",
        "                    current = current.find_next_sibling()\n",
        "                \n",
        "                if content:\n",
        "                    content_parts.append(f\"{heading_text}\\n{' '.join(content)}\")\n",
        "        \n",
        "        if content_parts:\n",
        "            return '\\n\\n'.join(content_parts), sections_found\n",
        "        return None, sections_found\n",
        "    \n",
        "    def extract_method3_fallback(self, soup: BeautifulSoup) -> Tuple[Optional[str], List[str]]:\n",
        "        \"\"\"\n",
        "        æ–¹æ³•3: Fallback - æ•´ä½“æå–åä½¿ç”¨æ­£åˆ™æˆªæ–­\n",
        "        \n",
        "        é€‚ç”¨äºéæ ‡å‡†é¡µé¢ç»“æ„\n",
        "        \"\"\"\n",
        "        sections_found = [\"[Fallbackæ¨¡å¼]\"]\n",
        "        \n",
        "        # å°è¯•å¤šç§å®¹å™¨é€‰æ‹©å™¨\n",
        "        article = soup.find('article') or \\\n",
        "                  soup.find('div', class_='article-content') or \\\n",
        "                  soup.find('div', id=re.compile(r'article|content', re.I)) or \\\n",
        "                  soup.find('main')\n",
        "        \n",
        "        if not article:\n",
        "            return None, sections_found\n",
        "        \n",
        "        # ç§»é™¤æ— ç”¨å…ƒç´ \n",
        "        for tag in article.find_all(['script', 'style', 'nav', 'footer', 'header', 'aside']):\n",
        "            tag.decompose()\n",
        "        \n",
        "        # ç§»é™¤æ‰€æœ‰æ’é™¤ç« èŠ‚åŠå…¶å†…å®¹\n",
        "        for heading in article.find_all(['h1', 'h2', 'h3', 'h4']):\n",
        "            if is_exclude_section(heading.get_text(strip=True)):\n",
        "                # ç§»é™¤è¯¥æ ‡é¢˜åŠæ‰€æœ‰åç»­å…„å¼Ÿå…ƒç´ \n",
        "                for sibling in list(heading.find_next_siblings()):\n",
        "                    sibling.decompose()\n",
        "                heading.decompose()\n",
        "                break\n",
        "        \n",
        "        text = article.get_text(separator=' ', strip=True)\n",
        "        \n",
        "        # å†æ¬¡ç”¨æ­£åˆ™ç¡®ä¿æ²¡æœ‰Referenceså†…å®¹\n",
        "        for pattern in [r'\\bReferences\\b', r'\\bBibliography\\b', r'\\bFootnotes\\b']:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                text = text[:match.start()]\n",
        "                break\n",
        "        \n",
        "        return (text.strip(), sections_found) if len(text) > 500 else (None, sections_found)\n",
        "    \n",
        "    def extract_full_text(self, pmcid: str) -> ExtractionResult:\n",
        "        \"\"\"\n",
        "        ä¸»æå–å‡½æ•°ï¼šä¾æ¬¡å°è¯•å¤šç§æ–¹æ³•æå–å…¨æ–‡\n",
        "        \n",
        "        Args:\n",
        "            pmcid: PMCæ–‡ç« IDï¼Œå¦‚ \"PMC12372713\" æˆ– \"12372713\"\n",
        "        \n",
        "        Returns:\n",
        "            ExtractionResult: åŒ…å«æå–ç»“æœçš„æ•°æ®ç±»\n",
        "        \"\"\"\n",
        "        # å‚æ•°éªŒè¯\n",
        "        if not pmcid:\n",
        "            return ExtractionResult(\n",
        "                pmcid=\"\", \n",
        "                status=ExtractionStatus.NO_PMCID, \n",
        "                error_message=\"PMCIDä¸ºç©º\"\n",
        "            )\n",
        "        \n",
        "        # æ ‡å‡†åŒ–PMCIDæ ¼å¼\n",
        "        pmcid = pmcid.strip()\n",
        "        if not pmcid.upper().startswith('PMC'):\n",
        "            pmcid = f\"PMC{pmcid}\"\n",
        "        pmcid = pmcid.upper()\n",
        "        \n",
        "        console.print(f\"\\n[bold blue]ğŸ“„ æŠ“å– {pmcid}...[/]\")\n",
        "        \n",
        "        try:\n",
        "            # è·å–HTML\n",
        "            html, error = self.fetch_html(pmcid)\n",
        "            if not html:\n",
        "                return ExtractionResult(\n",
        "                    pmcid=pmcid, \n",
        "                    status=ExtractionStatus.FAILED, \n",
        "                    error_message=error or \"æ— æ³•è·å–HTML\"\n",
        "                )\n",
        "            \n",
        "            soup = BeautifulSoup(html, 'html.parser')\n",
        "            \n",
        "            # æ–¹æ³•1: main-article-bodyç²¾ç¡®æå–ï¼ˆæ¨èï¼‰\n",
        "            console.print(\"[cyan]  æ–¹æ³•1: main-article-bodyç²¾ç¡®æå–[/]\")\n",
        "            text, sections = self.extract_method1_main_body(soup)\n",
        "            if text and len(text) > 1000:\n",
        "                clean = clean_text_v2(text)\n",
        "                console.print(f\"[green]  âœ“ æ–¹æ³•1æˆåŠŸ ({len(sections)}ç« èŠ‚, {len(clean)}å­—ç¬¦)[/]\")\n",
        "                return ExtractionResult(\n",
        "                    pmcid=pmcid, \n",
        "                    status=ExtractionStatus.SUCCESS,\n",
        "                    full_text=clean, \n",
        "                    full_text_chunks=chunk_text_v2(clean),\n",
        "                    method_used=\"method1_main_body\", \n",
        "                    sections_found=sections,\n",
        "                    char_count=len(clean),\n",
        "                    word_count=len(clean.split())\n",
        "                )\n",
        "            \n",
        "            # æ–¹æ³•2: æ ‡é¢˜å®šä½æå–\n",
        "            console.print(\"[cyan]  æ–¹æ³•2: æ ‡é¢˜å®šä½æå–[/]\")\n",
        "            text, sections = self.extract_method2_heading(soup)\n",
        "            if text and len(text) > 1000:\n",
        "                clean = clean_text_v2(text)\n",
        "                console.print(f\"[green]  âœ“ æ–¹æ³•2æˆåŠŸ ({len(sections)}ç« èŠ‚, {len(clean)}å­—ç¬¦)[/]\")\n",
        "                return ExtractionResult(\n",
        "                    pmcid=pmcid, \n",
        "                    status=ExtractionStatus.SUCCESS,\n",
        "                    full_text=clean, \n",
        "                    full_text_chunks=chunk_text_v2(clean),\n",
        "                    method_used=\"method2_heading\", \n",
        "                    sections_found=sections,\n",
        "                    char_count=len(clean),\n",
        "                    word_count=len(clean.split())\n",
        "                )\n",
        "            \n",
        "            # æ–¹æ³•3: Fallback\n",
        "            console.print(\"[yellow]  æ–¹æ³•3: Fallbackæå–[/]\")\n",
        "            text, sections = self.extract_method3_fallback(soup)\n",
        "            if text and len(text) > 500:\n",
        "                clean = clean_text_v2(text)\n",
        "                console.print(f\"[yellow]  âš  FallbackæˆåŠŸ ({len(clean)}å­—ç¬¦)[/]\")\n",
        "                return ExtractionResult(\n",
        "                    pmcid=pmcid, \n",
        "                    status=ExtractionStatus.PARTIAL,\n",
        "                    full_text=clean, \n",
        "                    full_text_chunks=chunk_text_v2(clean),\n",
        "                    method_used=\"method3_fallback\", \n",
        "                    sections_found=sections,\n",
        "                    char_count=len(clean),\n",
        "                    word_count=len(clean.split())\n",
        "                )\n",
        "            \n",
        "            # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥\n",
        "            return ExtractionResult(\n",
        "                pmcid=pmcid, \n",
        "                status=ExtractionStatus.FAILED, \n",
        "                error_message=\"æ‰€æœ‰æå–æ–¹æ³•å‡å¤±è´¥ï¼Œé¡µé¢ç»“æ„å¯èƒ½ä¸æ ‡å‡†\"\n",
        "            )\n",
        "            \n",
        "        except Exception as e:\n",
        "            console.print(f\"[red]  âœ— å¼‚å¸¸: {str(e)}[/]\")\n",
        "            return ExtractionResult(\n",
        "                pmcid=pmcid, \n",
        "                status=ExtractionStatus.FAILED, \n",
        "                error_message=f\"å¼‚å¸¸: {str(e)}\\n{traceback.format_exc()[:500]}\"\n",
        "            )\n",
        "\n",
        "# ============== ä¾¿æ·å‡½æ•° ==============\n",
        "\n",
        "def fetch_pmc_full_text_v2(pmcid: str, driver=None) -> Dict:\n",
        "    \"\"\"\n",
        "    ä¾¿æ·å‡½æ•°ï¼šè·å–PMCå…¨æ–‡ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰\n",
        "    \n",
        "    Args:\n",
        "        pmcid: PMCæ–‡ç« ID\n",
        "        driver: å¯é€‰çš„Selenium driverï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰\n",
        "    \n",
        "    Returns:\n",
        "        Dict: åŒ…å« pmcid, full_text, full_text_chunks, status, error ç­‰å­—æ®µ\n",
        "    \"\"\"\n",
        "    scraper = PMCScraperV2(use_selenium=False)\n",
        "    result = scraper.extract_full_text(pmcid)\n",
        "    \n",
        "    return {\n",
        "        'pmcid': result.pmcid,\n",
        "        'full_text': result.full_text,\n",
        "        'full_text_chunks': result.full_text_chunks,\n",
        "        'status': result.status.value,\n",
        "        'error': result.error_message,\n",
        "        'method': result.method_used,\n",
        "        'sections': result.sections_found,\n",
        "        'char_count': result.char_count,\n",
        "        'word_count': result.word_count\n",
        "    }\n",
        "\n",
        "# ============== æµ‹è¯•å‡½æ•° ==============\n",
        "\n",
        "def test_pmc_scraper(pmcid: str = \"PMC12372713\"):\n",
        "    \"\"\"\n",
        "    æµ‹è¯•æŠ“å–æ•ˆæœ\n",
        "    \n",
        "    ä½¿ç”¨ç¤ºä¾‹:\n",
        "        test_pmc_scraper(\"PMC12372713\")\n",
        "        test_pmc_scraper(\"12372713\")  # ä¹Ÿå¯ä»¥ä¸å¸¦PMCå‰ç¼€\n",
        "    \"\"\"\n",
        "    console.print(f\"\\n[bold magenta]{'='*60}[/]\")\n",
        "    console.print(f\"[bold magenta]ğŸ§ª æµ‹è¯•PMCæŠ“å–å™¨ v2.1 - {pmcid}[/]\")\n",
        "    console.print(f\"[bold magenta]{'='*60}[/]\")\n",
        "    \n",
        "    scraper = PMCScraperV2(use_selenium=False)\n",
        "    result = scraper.extract_full_text(pmcid)\n",
        "    \n",
        "    console.print(f\"\\n[bold]ğŸ“Š æŠ“å–ç»“æœ:[/]\")\n",
        "    console.print(f\"  PMCID:  {result.pmcid}\")\n",
        "    console.print(f\"  çŠ¶æ€:   {result.status.value}\")\n",
        "    console.print(f\"  æ–¹æ³•:   {result.method_used}\")\n",
        "    console.print(f\"  ç« èŠ‚:   {result.sections_found}\")\n",
        "    \n",
        "    if result.full_text:\n",
        "        console.print(f\"  å­—ç¬¦æ•°: {result.char_count:,}\")\n",
        "        console.print(f\"  è¯æ•°:   {result.word_count:,}\")\n",
        "        console.print(f\"  åˆ†å—æ•°: {len(result.full_text_chunks)}\")\n",
        "        \n",
        "        console.print(f\"\\n[bold]ğŸ“ æ–‡æœ¬é¢„è§ˆ (å‰800å­—ç¬¦):[/]\")\n",
        "        console.print(\"-\" * 60)\n",
        "        console.print(result.full_text[:800] + \"...\")\n",
        "        console.print(\"-\" * 60)\n",
        "        \n",
        "        console.print(f\"\\n[bold]ğŸ“ æ–‡æœ¬æœ«å°¾ (å500å­—ç¬¦):[/]\")\n",
        "        console.print(\"-\" * 60)\n",
        "        console.print(\"...\" + result.full_text[-500:])\n",
        "        console.print(\"-\" * 60)\n",
        "        \n",
        "        # éªŒè¯æ˜¯å¦åŒ…å«References\n",
        "        tail = result.full_text.lower()[-1000:]\n",
        "        if any(kw in tail for kw in ['references', 'bibliography', 'footnotes']):\n",
        "            console.print(\"[red]âš  è­¦å‘Š: æ–‡æœ¬æœ«å°¾å¯èƒ½åŒ…å«éæ­£æ–‡å†…å®¹![/]\")\n",
        "        else:\n",
        "            console.print(\"[green]âœ“ éªŒè¯é€šè¿‡: æœªæ£€æµ‹åˆ°References/Footnotesç­‰å†…å®¹[/]\")\n",
        "    else:\n",
        "        console.print(f\"\\n[red]  é”™è¯¯: {result.error_message}[/]\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# ============== ç›´æ¥è¿è¡Œæµ‹è¯• ==============\n",
        "# å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šæ¥è¿è¡Œæµ‹è¯•\n",
        "# test_pmc_scraper(\"PMC12372713\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf0147b1-a955-4e26-a8dc-6a14dc296c5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define prompts used for extracting data\n",
        "\n",
        "def get_prompts():\n",
        "    prompt1 = \"\"\"\n",
        "You are a medical research assistant specializing in the study of clinical risk prediction models. You assist users in collecting information and providing answers by identifying and analyzing medical information, statistical data, and prediction models within text. \n",
        "[Extracted text]\n",
        "Please analyze the above text and extract detailed information related to the study design and its key parameters. Specifically, extract the following elements.\n",
        "1. Study Type: Indicate whether the data used in the research comes from a prospective study (cohort study) or a retrospective study (case-control study).\n",
        "2. Disease Name: Specify the target disease studied in the text.\n",
        "3. Data Sources: Extract details about the primary data source, such as the name of the dataset, or the specific institution or public database from which the data were obtained.\n",
        "4. External Validation: Determine whether the study includes an external validation set, which refers to a dataset originating from a different source than the model development set. All subsequent references to the validation set refer specifically to external validation, not internal test sets.\n",
        "5. Date Range: Specify the dates of the collected participant data for model development and model external validation, including the start and end dates.\n",
        "6. Follow-up Time: Specify the follow-up time of the cohort for the model development stage, if applicable.\n",
        "7. Sample Characteristics: Indicate whether the included samples had a specific disease history, underwent particular surgeries or treatments, or were in a certain condition (e.g., pregnancy, smoking).\n",
        "8. Sample Sizes: Extract the sample sizes for the development set (including training set and test set) and external validation set, if mentioned.\n",
        "9. Case and Control Numbers: What are the numbers of cases and controls in the development set? What are the numbers of cases and controls in the external validation set? Perform inference and calculation as much as possible based on the provided sample data.\n",
        "10. Participant Statistics: Extract demographic information about participants in the development and external validation stages, including: the number of female participants; the number of male participants. Perform inference and calculation as much as possible based on the provided sample data.\n",
        "11. Extract age information about participants in the development and external validation stages, including the age range of participants; the average age and standard deviation of participants; the median age and interquartile range of participants.\n",
        "12. The racial or ethnic composition of participants in the development and external validation stages.\n",
        "    \"\"\"\n",
        "    prompt2 = \"\"\"\n",
        "Continue extracting the following details related to the prediction models from the research. If the paper constructs or evaluates multiple models, please extract the following information for all models.\n",
        "1. Model Type: Identify the type of prediction models used (e.g., logistic regression, random forests, deep learning).\n",
        "2. Prediction Variables: List the variables used in every prediction model.\n",
        "3. Report the AUC values, including confidence intervals, for the development stage and the validation stage.\n",
        "4. Report the C-index values, including confidence intervals, for the development stage and the validation stage.\n",
        "5. Report accuracy values, including confidence intervals, for the development stage and the validation stage.\n",
        "6. Report F1 scores, including confidence intervals, for the development stage and the validation stage.\n",
        "7. Determine whether calibration values are reported for the development stage and the validation stage, such as calibration curve, Hosmerâ€“Lemeshow test, Brier Score, or expected vs. observed outcomes. If available, please report them.\n",
        "8. Nomogram Development: Indicate whether a nomogram was constructed based on the model.\n",
        "9. Use of TRIPOD Guidelines: State whether the study followed the TRIPOD guidelines, if applicable.\n",
        "    \"\"\"\n",
        "    prompt3 = \"\"\"\n",
        "Please format the extracted information into two columns: one column for the field and one column for the value, ensuring that each field corresponds to only one value. Use the following fields to create the table. This table consists of 25 fields.\n",
        "1. Study Type: e.g., prospective study, retrospective study. If it is not mentioned in the text, answer NA.\n",
        "2. Disease Name: e.g. lung cancer.\n",
        "3. External Validation: Answer Yes or No.\n",
        "4. Date Range in the Development Set: e.g., 1992â€“2008. If it is not mentioned in the text, answer NA.\n",
        "5. Date Range in the Validation Set: e.g., 1992â€“2008. If this study lacks external validation, answer N/A.\n",
        "6. Median Follow-up Time (Years) and IQR: e.g., 2.95 years (IQR: 1.71â€“4.83). If it is not mentioned in the text, answer NA.\n",
        "7. Mean Follow-up Time (Years) and Standard Error: e.g., 2.95 years (SD: 0.71). If it is not mentioned in the text, answer NA.\n",
        "8. Data Sources: e.g., the Kaiser Permanente Washington Breast Cancer Surveillance Consortium (BCSC) registry, the GEO database, the Cancer Screening Program in Urban China (CanSPUC) conducted in Henan Province. If it is not mentioned in the text, answer NA.\n",
        "9. Sample Characteristics: e.g., patients with a history of diabetes, pregnant women, ever-smokers. If it is not mentioned in the text, answer N/A.\n",
        "10. Number of Cases in the Development Set: e.g., 5,323. If it is not mentioned in the text, answer NA.\n",
        "11. Number of Controls in the Development Set: e.g., 5,323. If it is not mentioned in the text, answer NA.\n",
        "12. Number of Cases in the External Validation Set: e.g., 5,323. If this study lacks external validation, answer N/A.\n",
        "13. Number of Controls in the External Validation Set: e.g., 5,323. If this study lacks external validation, answer N/A.\n",
        "14. Number of Female Participants (Development): e.g., 86. If it is not mentioned in the text, answer NA.\n",
        "15. Number of Female Participants (External Validation): e.g., 86. If this study lacks external validation, answer N/A.\n",
        "16. Number of Male Participants (Development): e.g., 545. If it is not mentioned in the text, answer NA.\n",
        "17. Number of Male Participants (External Validation): e.g., 545. If this study lacks external validation, answer N/A.\n",
        "18. Age Range (Development): e.g., 22â€“60. If it is not mentioned in the text, answer NA.\n",
        "19. Age Range (External Validation): e.g., 22â€“60. If this study lacks external validation, answer N/A.\n",
        "20. Average Age and Standard Deviation (Development): e.g., 74.2 years (SD = 8.2). If it is not mentioned in the text, answer NA.\n",
        "21. Average Age and Standard Deviation (External Validation): e.g., 74.2 years (SD = 8.2). If this study lacks external validation, answer N/A.\n",
        "22. Median Age (Development) and IQR: e.g., 43.2 years (IQR: 32.43â€“64.83). If it is not mentioned in the text, answer NA.\n",
        "23. Median Age (External Validation) and IQR: e.g., 43.2 years (IQR: 32.43â€“64.83). If this study lacks external validation, answer N/A.\n",
        "24. Racial/Ethnic Composition (Development): e.g., White: 80.4%, Asian: 8.8%, Black: 3.9%, Other or multiple races: 5.3%, Unknown: 1.5%. If it is not mentioned in the text, answer NA.\n",
        "25. Racial/Ethnic Composition (External Validation): e.g., White: 80.4%, Asian: 8.8%, Black: 3.9%, Other or multiple races: 5.3%, Unknown: 1.5%. If this study lacks external validation, answer N/A.\n",
        "    \"\"\"\n",
        "    prompt4 = \"\"\"\n",
        "Please extract and format the following information for the models in the study. Organize the data into a two-column or multi-column table, with one column for the field name (e.g., Model Name, Model Type, etc.) and the remaining columns for values. Each column corresponds to one model's values. If there are multiple models, a multi-column table is needed. Each field should occupy only one row in the table. This table consists of 10 fields.\n",
        "1. Model Name: e.g., Tyrer-Cuzick Model, LiFeCRC Score.\n",
        "2. Model Type: e.g., logistic regression, random forest, support vector machine.\n",
        "3. Prediction Variables: e.g., age, gender, education, smoking status, blood pressure medication, prevalent stroke.\n",
        "4. AUC Values: e.g., 0.767 (95% CI: 0.749â€“0.786). If a validation stage is present, prioritize reporting the values from the validation stage.\n",
        "5. C-index Values: e.g., 0.767 (95% CI: 0.749â€“0.786). If a validation stage is present, prioritize reporting the values from the validation stage.\n",
        "6. Accuracy: e.g., 74.8% (95% CI: 71.30% - 78.30%). If a validation stage is present, prioritize reporting the values from the validation stage.\n",
        "7. F1-score: e.g., 74.8% (95% CI: 71.30% - 78.30%). If a validation stage is present, prioritize reporting the values from the validation stage.\n",
        "8. Calibration Values: e.g., Hosmer-Lemeshow (HL) test p-value = 0.428; O/E ratio ranged from 0.79 to 1.22, or a brief description. If a validation stage is present, prioritize reporting the values from the validation stage.\n",
        "9. Nomogram Application: Answer Yes or No.\n",
        "10. Use of TRIPOD Guidelines: Answer Yes or No.\n",
        "    \"\"\"\n",
        "    return [prompt1, prompt2, prompt3, prompt4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ffdce90-3f15-4023-9076-5cf9ca58c2b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Structured data storage\n",
        "import re\n",
        "from prompts import get_prompts\n",
        "from utils import chat_completion\n",
        "from rich.console import Console\n",
        "\n",
        "console = Console()\n",
        "\n",
        "# Define the fields for sample and model data\n",
        "sample_fields = [\n",
        "    \"Study Type\",\n",
        "    \"Disease Name\",\n",
        "    \"External Validation\",\n",
        "    \"Date Range in the Development Set\",\n",
        "    \"Date Range in the Validation Set\",\n",
        "    \"Median Follow-up Time (Years) and IQR\",\n",
        "    \"Mean Follow-up Time (Years) and Standard Error\",\n",
        "    \"Data Sources\",\n",
        "    \"Sample Characteristics\",\n",
        "    \"Number of Cases in the Development Set\",\n",
        "    \"Number of Controls in the Development Set\",\n",
        "    \"Number of Cases in the Validation Set\",\n",
        "    \"Number of Controls in the Validation Set\",\n",
        "    \"Number of Female Participants (Development)\",\n",
        "    \"Number of Female Participants (Validation)\",\n",
        "    \"Number of Male Participants (Development)\",\n",
        "    \"Number of Male Participants (Validation)\",\n",
        "    \"Age Range (Development)\",\n",
        "    \"Age Range (Validation)\",\n",
        "    \"Average Age and Standard Deviation (Development)\",\n",
        "    \"Average Age and Standard Deviation (Validation)\",\n",
        "    \"Median Age (Development) and IQR\",\n",
        "    \"Median Age (Validation) and IQR\",\n",
        "    \"Racial/Ethnic Composition (Development)\",\n",
        "    \"Racial/Ethnic Composition (Validation)\"\n",
        "]\n",
        "\n",
        "model_fields = [\n",
        "    \"Model Name\",\n",
        "    \"Model Type\",\n",
        "    \"Prediction Variables\",\n",
        "    \"AUC Values\",\n",
        "    \"C-index Values\",\n",
        "    \"Accuracy\",\n",
        "    \"F1-score\",\n",
        "    \"Calibration Values\",\n",
        "    \"Nomogram Application\",\n",
        "    \"Use of TRIPOD Guidelines\"\n",
        "]\n",
        "\n",
        "# Process input text using the prompts and obtain responses from Grok-3\n",
        "def process_input(input_text, prompts):\n",
        "    responses = []\n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt.replace(\"[Extracted text]\", input_text)}]\n",
        "        console.print(f\"[italic yellow]Processing Prompt {i}...[/]\")\n",
        "        response = chat_completion(messages)\n",
        "        if response:\n",
        "            responses.append(response)\n",
        "            console.print(f\"\\n[bold magenta]Response {i}:[/]\")\n",
        "            console.print(response)\n",
        "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "        else:\n",
        "            console.print(f\"[bold red]Prompt {i} processing failed[/]\")\n",
        "    return responses\n",
        "\n",
        "# Extract sample data from the response based on predefined fields\n",
        "def extract_sample_data(response_sample, pmid):\n",
        "    data_dict = {field: \"\" for field in sample_fields}\n",
        "    data_dict[\"PMID\"] = pmid\n",
        "    lines_sample = response_sample.splitlines()\n",
        "    \n",
        "    for line in lines_sample:\n",
        "        match = re.match(r'\\s*\\d+\\.\\s+([^0-9].*?)\\s{2,}(.*)', line.strip())\n",
        "        if match:\n",
        "            field, value = match.groups()\n",
        "            for sample_field in sample_fields:\n",
        "                if sample_field in field:\n",
        "                    data_dict[sample_field] = value\n",
        "                    break\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "# Extract model data from the response based on predefined fields, handling multiple models\n",
        "def extract_model_data(response_model, pmid):\n",
        "    model_data_list = []\n",
        "    lines_model = response_model.splitlines()\n",
        "    \n",
        "    current_model = {field: \"\" for field in model_fields}\n",
        "    current_model[\"PMID\"] = pmid\n",
        "    \n",
        "    for line in lines_model:\n",
        "        match = re.match(r'\\s*\\d+\\.\\s+([^0-9].*?)\\s{2,}(.*)', line.strip())\n",
        "        if match:\n",
        "            field, value = match.groups()\n",
        "            if \"Model Name\" in field and current_model[\"Model Name\"]:\n",
        "                # New model detected, save the current one and start a new one\n",
        "                model_data_list.append(current_model)\n",
        "                current_model = {field: \"\" for field in model_fields}\n",
        "                current_model[\"PMID\"] = pmid\n",
        "            for model_field in model_fields:\n",
        "                if model_field in field:\n",
        "                    current_model[model_field] = value\n",
        "                    break\n",
        "    \n",
        "    # Append the last model\n",
        "    if current_model[\"Model Name\"] or any(current_model[field] for field in model_fields if field != \"PMID\"):\n",
        "        model_data_list.append(current_model)\n",
        "    \n",
        "    # If no models were identified, return a single empty model with PMID\n",
        "    if not model_data_list:\n",
        "        model_data_list.append({field: \"\" for field in model_fields})\n",
        "        model_data_list[0][\"PMID\"] = pmid\n",
        "    \n",
        "    return model_data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d56eccd-eccb-4573-8670-2776151c98e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# main progrom\n",
        "\n",
        "import csv\n",
        "from rich.console import Console\n",
        "\n",
        "# Configuration\n",
        "console = Console()\n",
        "\n",
        "def main():\n",
        "    console.print(\"[bold green]Welcome to the Clinical Risk Prediction Model Analysis Tool[/]\")\n",
        "    txt_file = 'PMID.TXT'\n",
        "    pmids = read_pmid_from_txt(txt_file)\n",
        "    if not pmids:\n",
        "        console.print(\"[bold red]Failed to read PMID[/]\")\n",
        "        return\n",
        "\n",
        "    with open('literature_data.csv', 'a', newline='', encoding='utf-8') as literature_csv, \\\n",
        "         open('sample_data.csv', 'a', newline='', encoding='utf-8') as sample_csv, \\\n",
        "         open('model_information.csv', 'a', newline='', encoding='utf-8') as model_csv:\n",
        "        \n",
        "        literature_writer = csv.DictWriter(literature_csv, fieldnames=literature_fields)\n",
        "        sample_writer = csv.DictWriter(sample_csv, fieldnames=sample_fields)\n",
        "        model_writer = csv.DictWriter(model_csv, fieldnames=model_fields)\n",
        "        \n",
        "        if literature_csv.tell() == 0:\n",
        "            literature_writer.writeheader()\n",
        "        if sample_csv.tell() == 0:\n",
        "            sample_writer.writeheader()\n",
        "        if model_csv.tell() == 0:\n",
        "            model_writer.writeheader()\n",
        "\n",
        "        for pmid in pmids:\n",
        "            console.print(f\"[italic yellow]Processing PMID: {pmid}...[/]\")\n",
        "            driver = setup_driver()\n",
        "            pubmed_data = fetch_pubmed_data(pmid, driver)\n",
        "            full_text_data = fetch_pmc_full_text(pubmed_data['pmcid'], driver)\n",
        "            full_text = full_text_data.get('full_text', \"\")\n",
        "            \n",
        "            literature_writer.writerow({\n",
        "                'PMID': pmid,\n",
        "                'Title': pubmed_data.get('title', ''),\n",
        "                'Authors': pubmed_data.get('authors', ''),\n",
        "                'First Author Last Affiliation Word': pubmed_data.get('first_author_last_affiliation_word', ''),\n",
        "                'DOI': pubmed_data.get('doi', ''),\n",
        "                'Keywords': pubmed_data.get('keywords', ''),\n",
        "                'Journal Name': pubmed_data.get('journal_name', ''),\n",
        "                'PMCID': pubmed_data.get('pmcid', ''),\n",
        "                'Full Text': full_text\n",
        "            })\n",
        "            \n",
        "            prompts = get_prompts()\n",
        "            prompt1_with_fulltext = prompts[0].replace(\"[input text]\", full_text)\n",
        "            responses = process_input(prompt1_with_fulltext, prompts)\n",
        "            \n",
        "            if len(responses) >= 4:\n",
        "                response_sample = responses[2]  # Prompt 3\n",
        "                response_model = responses[3]  # Prompt 4\n",
        "                \n",
        "                sample_data = extract_sample_data(response_sample, pmid)\n",
        "                model_data = extract_model_data(response_model, pmid)\n",
        "                \n",
        "                sample_writer.writerow(sample_data)\n",
        "                model_writer.writerow(model_data)\n",
        "            \n",
        "            driver.quit()\n",
        "            print(f\"Information related to PMID {pmid} has been saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c96442d-49af-46a7-a03d-deed3c6f684c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Double-LLM-Validation\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def get_pmcid_from_pmid(pmid):\n",
        "    try:\n",
        "        url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={pmid}&rettype=xml\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        root = ET.fromstring(response.content)\n",
        "        pmcid_elem = root.find('.//PMID[@Version=\"1\"]/..//ArticleId[@IdType=\"pmc\"]')\n",
        "        if pmcid_elem is not None:\n",
        "            return pmcid_elem.text\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching PMCID for PMID {pmid}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def disagreement_resolution(model, model_key, agreement_filepath):\n",
        "    df = pd.read_excel(agreement_filepath)\n",
        "\n",
        "    variables = df[\"Variable\"]\n",
        "    gr = df[\"GROK_Responses\"] \n",
        "    cr = df[\"CLAUDE_Responses\"]\n",
        "    ad = df[\"Agree(A)/Disagree(D)\"]\n",
        "    pmids = df[\"PMID\"]\n",
        "\n",
        "    allresponses_list = []\n",
        "    count = 0\n",
        "\n",
        "    for i in range(len(cr)):\n",
        "        if ad[i] == \"D\":  # Disagreement Check\n",
        "            pmid = pmids[i] \n",
        "            pmcid = get_pmcid_from_pmid(pmid)\n",
        "            if pmcid is None:\n",
        "                print(f\"PMCID not found for PMID {pmid}, skipping.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing PMID {pmid} \\t Count: {count}\\n\")\n",
        "            count += 1\n",
        "\n",
        "            # Prompt Design\n",
        "            var = str(variables[i])\n",
        "            if model == \"claude-sonnet-4-20250514\":\n",
        "                val = str(gr[i])  # GROK_Responses\n",
        "            else:\n",
        "                val = str(cr[i])\n",
        "            p = \"For the given text, LLM generate the response = [\" + val + \"] for the variable = [\" + var + \"]. Verify if the response for the given variable generated by LLM is correct or incorrect. If the response is incorrect, then generate the correct response (as short and precise as possible).\"\n",
        "\n",
        "            # Acquiring text\n",
        "            try:\n",
        "                url = f\"https://pmc.ncbi.nlm.nih.gov/articles/{pmcid}/\"\n",
        "                response = requests.get(url)\n",
        "                response.raise_for_status()  # Check status\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                abstract_section = soup.find(\"section\", {\"class\": \"abstract\", \"id\": \"abstract1\"})\n",
        "                references_section = soup.find(\"h2\", {\"class\": \"pmc_sec_title\"}, string=\"References\")\n",
        "                if abstract_section and references_section:\n",
        "                    content = \"\"\n",
        "                    current = abstract_section.find_next()\n",
        "                    while current and current != references_section:\n",
        "                        if current.name == \"section\":\n",
        "                            content += current.get_text(separator=\" \", strip=True) + \"\\n\"\n",
        "                        current = current.find_next()\n",
        "                else:\n",
        "                    content = \"Failed to extract content between Abstract and References.\"\n",
        "                print(f\"Extracted Content Length: {len(content)} characters\\n\")\n",
        "            except Exception as e:\n",
        "                content = f\"Error fetching content for PMCID {pmcid}: {str(e)}\"\n",
        "                print(content)\n",
        "\n",
        "            allresponses = \"PMID: \" + str(pmid) + \"\\n\"\n",
        "            allresponses += \"PMCID: \" + pmcid + \"\\n\"\n",
        "            allresponses += \"Variable Name: \" + var + \"\\n\"\n",
        "            allresponses += \"GROK Response: \" + str(gr[i]) + \"\\n\"  # GROK Response\n",
        "            allresponses += \"Claude Response: \" + str(cr[i]) + \"\\n\\n\"\n",
        "            allresponses += \"Verification from the Text...\\n\"\n",
        "\n",
        "            # Processing text\n",
        "            if model == \"claude\":\n",
        "                response = claude_function(content, p, model_key, model)\n",
        "                response = str(response)\n",
        "                time.sleep(30)  # Bypass TPM restrictions\n",
        "            else:\n",
        "                response = gpt_function(content, p, model_key, model)\n",
        "            allresponses = allresponses + \"\\n\" + response\n",
        "\n",
        "            print(\"\\n\")\n",
        "            allresponses_list.append(allresponses)\n",
        "\n",
        "    ff = \"DisagreementResolution_by_\" + model\n",
        "    write_raw_responses(allresponses_list, ff)\n",
        "\n",
        "a = input(\"Do you to execute Disagreement Resolution Module? Y/N\")\n",
        "if a == \"Y\" or a == \"y\":\n",
        "    model = input(\"Enter the Model Name\")\n",
        "    key = input(\"Enter the Key\")\n",
        "    agreement_filepath = input(\"Enter the Path to the Annotated Agreement Matching File\")\n",
        "    disagreement_resolution(model, key, agreement_filepath)\n",
        "else:\n",
        "    print(\"You Choose Not to Execute Agreement/Disagreement Module.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
